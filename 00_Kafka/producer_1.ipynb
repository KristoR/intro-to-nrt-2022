{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fastavro\n",
    "\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import getpass\n",
    "import psycopg2\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"\n",
    "    Reports the failure or success of a message delivery.\n",
    "    Args:\n",
    "        err (KafkaError): The error that occurred on None on success.\n",
    "        msg (Message): The message that was produced or failed.\n",
    "    Note:\n",
    "        In the delivery report callback the Message.key() and Message.value()\n",
    "        will be the binary format as encoded by any configured Serializers and\n",
    "        not the same object that was passed to produce().\n",
    "        If you wish to pass the original object(s) for key and value to delivery\n",
    "        report callback we recommend a bound callback or lambda where you pass\n",
    "        the objects along.\n",
    "    \"\"\"\n",
    "    if err is not None:\n",
    "        print(\"Delivery failed for User record {}: {}\".format(msg.key(), err))\n",
    "        return\n",
    "    print('User record {} successfully produced to {} [{}] at offset {}'.format(\n",
    "        msg.key(), msg.topic(), msg.partition(), msg.offset()))\n",
    "\n",
    "# Create a Schema Registry reference (client)\n",
    "sr = SchemaRegistryClient({\"url\": 'http://localhost:8081'})\n",
    "\n",
    "# FYI: https://avro.apache.org/docs/current/spec.html\n",
    "schema_str = \"\"\"{\n",
    "    \"name\": \"Order\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"order_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"product_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"user_id\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Create an Avro Serializer, connecting the Schema Registry and the schema definition\n",
    "avro_serializer = AvroSerializer(sr,schema_str)\n",
    "\n",
    "# Producer configuration: also including how we serialize values (can also serialize keys)\n",
    "producer_conf = {'bootstrap.servers': \"localhost:19092,localhost:29092\",\n",
    "                    'value.serializer': avro_serializer}\n",
    "\n",
    "# Create the producer\n",
    "producer = SerializingProducer(producer_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueSerializationError",
     "evalue": "KafkaError{code=_VALUE_SERIALIZATION,val=-161,str=\"must be string on field user_id\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be string",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\serializing_producer.py\u001b[0m in \u001b[0;36mproduce\u001b[1;34m(self, topic, key, value, partition, on_delivery, timestamp, headers)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_serializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\schema_registry\\avro.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj, ctx)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;31m# write the record to the rest of the buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mschemaless_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parsed_schema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.schemaless_writer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_record\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mfastavro\\_write.pyx\u001b[0m in \u001b[0;36mfastavro._write.write_utf8\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be string on field user_id",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueSerializationError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26672/2919609006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"user_id\": 1}\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mproducer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sales_schema\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_delivery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelivery_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Error: user_id should be string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\serializing_producer.py\u001b[0m in \u001b[0;36mproduce\u001b[1;34m(self, topic, key, value, partition, on_delivery, timestamp, headers)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_serializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueSerializationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         super(SerializingProducer, self).produce(topic, value, key,\n",
      "\u001b[1;31mValueSerializationError\u001b[0m: KafkaError{code=_VALUE_SERIALIZATION,val=-161,str=\"must be string on field user_id\"}"
     ]
    }
   ],
   "source": [
    "sample = {\"order_id\": 100,\n",
    "\"product_id\": 20,\n",
    "\"user_id\": 1}\n",
    "\n",
    "producer.produce(topic=\"sales_schema\", value=sample, on_delivery=delivery_report)\n",
    "\n",
    "# Error: user_id should be string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {\"order_id\": 100,\n",
    "\"product_id\": 20,\n",
    "\"user_id\": \"1\"}\n",
    "\n",
    "producer.produce(topic=\"sales_schema\", value=sample, on_delivery=delivery_report)\n",
    "\n",
    "# OK --> now schema is actually registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema changes, we now want to have customer_id which is integer. \n",
    "# There is no more user_id\n",
    "\n",
    "schema_str = \"\"\"{\n",
    "    \"name\": \"Order\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"order_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"product_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"customer_id\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_serializer = AvroSerializer(sr,schema_str)\n",
    "\n",
    "producer_conf = {'bootstrap.servers': \"localhost:19092,localhost:29092\",\n",
    "                    'value.serializer': avro_serializer}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"\"\n",
    "port = \"\"\n",
    "db = \"\"\n",
    "user = \"\"\n",
    "pw = \"\"\n",
    "\n",
    "engine = create_engine(f'postgresql+psycopg2://{user}:{pw}@{host}:{port}/{db}')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT order_id, product_id, customer_id FROM practice.sales_order\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueSerializationError",
     "evalue": "KafkaError{code=_VALUE_SERIALIZATION,val=-161,str=\"Schema being registered is incompatible with an earlier schema for subject \"sales_schema-value\", details: [Incompatibility{type:READER_FIELD_MISSING_DEFAULT_VALUE, location:/fields/2, message:customer_id, reader:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"customer_id\",\"type\":\"long\"}]}, writer:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"user_id\",\"type\":\"string\"}]}}] (HTTP status code 409, SR code 409)\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSchemaRegistryError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\serializing_producer.py\u001b[0m in \u001b[0;36mproduce\u001b[1;34m(self, topic, key, value, partition, on_delivery, timestamp, headers)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_serializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\schema_registry\\avro.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj, ctx)\u001b[0m\n\u001b[0;32m    249\u001b[0m                     \u001b[1;31m# the initial registration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                     self._schema_id = self._registry.register_schema(subject,\n\u001b[0m\u001b[0;32m    251\u001b[0m                                                                      self._schema)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py\u001b[0m in \u001b[0;36mregister_schema\u001b[1;34m(self, subject_name, schema)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         response = self._rest_client.post(\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;34m'subjects/{}/versions'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_urlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py\u001b[0m in \u001b[0;36mpost\u001b[1;34m(self, url, body, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\schema_registry\\schema_registry_client.py\u001b[0m in \u001b[0;36msend_request\u001b[1;34m(self, url, method, body, query)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             raise SchemaRegistryError(response.status_code,\n\u001b[0m\u001b[0;32m    175\u001b[0m                                       \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSchemaRegistryError\u001b[0m: Schema being registered is incompatible with an earlier schema for subject \"sales_schema-value\", details: [Incompatibility{type:READER_FIELD_MISSING_DEFAULT_VALUE, location:/fields/2, message:customer_id, reader:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"customer_id\",\"type\":\"long\"}]}, writer:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"user_id\",\"type\":\"string\"}]}}] (HTTP status code 409, SR code 409)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueSerializationError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26672/3211329589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mproducer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sales_schema\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_delivery\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelivery_report\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\confluent_kafka\\serializing_producer.py\u001b[0m in \u001b[0;36mproduce\u001b[1;34m(self, topic, key, value, partition, on_delivery, timestamp, headers)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_serializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueSerializationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         super(SerializingProducer, self).produce(topic, value, key,\n",
      "\u001b[1;31mValueSerializationError\u001b[0m: KafkaError{code=_VALUE_SERIALIZATION,val=-161,str=\"Schema being registered is incompatible with an earlier schema for subject \"sales_schema-value\", details: [Incompatibility{type:READER_FIELD_MISSING_DEFAULT_VALUE, location:/fields/2, message:customer_id, reader:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"customer_id\",\"type\":\"long\"}]}, writer:{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"user_id\",\"type\":\"string\"}]}}] (HTTP status code 409, SR code 409)\"}"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    event = dict(r.items())\n",
    "    producer.produce(topic=\"sales_schema\", value=event, on_delivery=delivery_report)\n",
    "\n",
    "# Error: our new schema is incompatible with old schema\n",
    "# (default compatibility is BACKWARD compatible)\n",
    "# It is OK to: \n",
    "# * Delete fields\n",
    "# * Add optional fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we delete the user_id and add optional field customer_id\n",
    "\n",
    "schema_str = \"\"\"{\n",
    "    \"name\": \"Order\",\n",
    "    \"type\": \"record\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"order_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"product_id\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"customer_id\",\n",
    "            \"type\": \"long\",\n",
    "            \"default\": -1\n",
    "        }\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# We need to create a new serializer and new producer with this new schema\n",
    "\n",
    "avro_serializer = AvroSerializer(sr,schema_str)\n",
    "\n",
    "producer_conf = {'bootstrap.servers': \"localhost:19092,localhost:29092\",\n",
    "                    'value.serializer': avro_serializer}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now everything works\n",
    " \n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT order_id, product_id, customer_id FROM practice.sales_order\"))\n",
    "\n",
    "for r in result:\n",
    "    event = dict(r.items())\n",
    "    producer.produce(topic=\"sales_schema\", value=event, on_delivery=delivery_report)\n",
    "\n",
    "# you can validate from terminal by creating a simple consumer:\n",
    "# docker exec --interactive --tty schema-registry kafka-avro-console-consumer --bootstrap-server broker-1:9092  --key-deserializer org.apache.kafka.common.serialization.StringDeserializer --topic sales_schema --from-beginning\n",
    "# you should see the first msesage with user_id and the subsequent messages with customer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sales-value', 'sales3-value', 'sales_schema-value']\n",
      "[1, 2]\n",
      "AVRO\n",
      "{\"type\":\"record\",\"name\":\"Order\",\"fields\":[{\"name\":\"order_id\",\"type\":\"long\"},{\"name\":\"product_id\",\"type\":\"long\"},{\"name\":\"user_id\",\"type\":\"string\"}]}\n"
     ]
    }
   ],
   "source": [
    "# List existing schemas:\n",
    "print(sr.get_subjects())\n",
    "# Get versions for a schema:\n",
    "print(sr.get_versions(\"sales_schema-value\"))\n",
    "\n",
    "# print type or string of old schema:\n",
    "old_schema = sr.get_version(\"sales_schema-value\", 1)\n",
    "print(old_schema.schema.schema_type)\n",
    "print(old_schema.schema.schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading material:\n",
    "# https://medium.com/slalom-technology/introduction-to-schema-registry-in-kafka-915ccf06b902\n",
    "# https://docs.confluent.io/platform/current/schema-registry/avro.html#compatibility-types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86a545e9c7c8a04a9f5ab582097b4968329114428a3fde0cf520c5aeef29910b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
